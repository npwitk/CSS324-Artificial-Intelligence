{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6884318",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4a8ff",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "PyTorch is an open-source machine learning library widely used for building and training deep learning models. Developed by Meta AI (formerly Facebook AI Research), PyTorch provides a flexible and efficient platform for **tensor computation**, **automatic differentiation**, and **neural network development**.\n",
    "\n",
    "PyTorch supports multiple types of devices, such as CPU and GPU, allowing computations to run efficiently on different hardware. It can automatically detect available devices and easily move tensors or models between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cc509e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd5900",
   "metadata": {},
   "source": [
    "# Tensors\n",
    "\n",
    "Tensors are the basic data type in PyTorch, similar to `ndarray` in NumPy.  \n",
    "They represent multi-dimensional arrays and support a wide range of mathematical operations.\n",
    "\n",
    "Tensors can be processed on both CPU and GPU, making them suitable for efficient numerical and deep learning computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ac6cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]])\n",
      "\n",
      "B = A + 2:\n",
      "tensor([[3, 4],\n",
      "        [5, 6]])\n",
      "\n",
      "A on mps:\n",
      "tensor([[1, 2],\n",
      "        [3, 4]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor\n",
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "print(\"A:\")\n",
    "print(A)\n",
    "\n",
    "# Basic tensor operation\n",
    "B = A + 2\n",
    "print(\"\\nB = A + 2:\")\n",
    "print(B)\n",
    "\n",
    "# Move tensor to the selected device\n",
    "A = A.to(device)\n",
    "print(f\"\\nA on {device}:\")\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d460ad",
   "metadata": {},
   "source": [
    "# Automatic Differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c489381d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[1.],\n",
      "        [3.]], device='mps:0', requires_grad=True)\n",
      "\n",
      "After Operations:\n",
      "tensor([[ 3.],\n",
      "        [11.]], device='mps:0', grad_fn=<AddBackward0>)\n",
      "\n",
      "After Averaging:\n",
      "tensor(7., device='mps:0', grad_fn=<MeanBackward0>)\n",
      "\n",
      "Gradients:\n",
      "tensor([[1.],\n",
      "        [3.]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor with requires_grad=True to track computation\n",
    "x = torch.tensor([[1.0], [3.0]], requires_grad=True, device=device)\n",
    "print(\"Original Tensor:\")\n",
    "print(x)\n",
    "\n",
    "# Perform some operations\n",
    "y = x**2 + 2\n",
    "print(\"\\nAfter Operations:\")\n",
    "print(y)\n",
    "\n",
    "out = y.mean()\n",
    "print(\"\\nAfter Averaging:\")\n",
    "print(out)\n",
    "\n",
    "# Perform backpropagation\n",
    "out.backward()\n",
    "print(\"\\nGradients:\")\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14714227",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e92ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bdee79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "basic_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(),                      # Convert PIL image to tensor\n",
    "    transforms.Lambda(lambda x: x.view(-1)),    # Flatten the image -> Convert it to vector\n",
    "    transforms.Lambda(lambda x: x / 255.0)      # Normalize pixel values to [0, 1]\n",
    "])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=basic_transforms,\n",
    "    download=True\n",
    ")\n",
    "# Split the train_dataset into train and validation sets\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=basic_transforms,\n",
    "    download=True\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6a8cb8",
   "metadata": {},
   "source": [
    "# Neural Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c7f6958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59aa96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "# Note that the softmax function is not needed in the last layer, as it is included in the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00419973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetWithRegularization(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.2):\n",
    "        super(SimpleNetWithRegularization, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25e87668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop with Early Stopping\n",
    "def train_model(model, train_loader, val_loader, loss_fn, \n",
    "                optimizer, epochs, device, \n",
    "                patience=5, min_delta=0.001):\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_steps = 0.0, 0\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            # Forward pass  \n",
    "            y_pred = model(x_batch)\n",
    "            # Compute loss\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Update weights\n",
    "            optimizer.step()\n",
    "            # Accumulate loss\n",
    "            train_loss += loss.item()\n",
    "            train_steps += 1\n",
    "        train_loss_avg = train_loss / train_steps\n",
    "        train_losses.append(train_loss_avg)\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss, val_steps = 0.0, 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                x_batch = x_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                y_pred = model(x_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "                val_loss += loss.item()\n",
    "                val_steps += 1\n",
    "        val_loss_avg = val_loss / val_steps\n",
    "        val_losses.append(val_loss_avg)\n",
    "        \n",
    "        # Early stopping logic\n",
    "        if val_loss_avg < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss_avg\n",
    "            patience_counter = 0\n",
    "            # Save the best model state\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"Epoch {epoch + 1:3d}/{epochs}, Loss: {train_loss_avg:.4f},\", \n",
    "                f\"Validation Loss: {val_loss_avg:.4f} (Best)\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"Epoch {epoch + 1:3d}/{epochs}, Loss: {train_loss_avg:.4f},\", \n",
    "                f\"Validation Loss: {val_loss_avg:.4f}\", \n",
    "                f\"(Patience: {patience_counter}/{patience})\")\n",
    "        \n",
    "        # Check for early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch + 1}\")\n",
    "            print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "            break\n",
    "    \n",
    "    # Restore the best model state\n",
    "    if best_model_state is not None:\n",
    "        # Save the best model state to a file\n",
    "        torch.save(best_model_state, 'best_model.pth')\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"Restored best model state\")\n",
    "    \n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54e0b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on test set and calculate accuracy\n",
    "def evaluate_model(model, test_loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    test_loss, test_steps = 0.0, 0\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            y_pred = model(x_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            test_loss += loss.item()\n",
    "            test_steps += 1\n",
    "            preds = y_pred.argmax(dim=1)\n",
    "            correct_predictions += (preds == y_batch).sum().item()\n",
    "    test_loss_avg = test_loss / test_steps\n",
    "    test_accuracy = correct_predictions / len(test_loader.dataset)\n",
    "    print(f\"Loss: {test_loss_avg:.4f}, Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8433df0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/20, Loss: 1.0494, Validation Loss: 0.5961 (Best)\n",
      "Epoch   2/20, Loss: 0.5192, Validation Loss: 0.4529 (Best)\n",
      "Epoch   3/20, Loss: 0.4196, Validation Loss: 0.3988 (Best)\n",
      "Epoch   4/20, Loss: 0.3737, Validation Loss: 0.3565 (Best)\n",
      "Epoch   5/20, Loss: 0.3405, Validation Loss: 0.3218 (Best)\n",
      "Epoch   6/20, Loss: 0.3114, Validation Loss: 0.3010 (Best)\n",
      "Epoch   7/20, Loss: 0.2868, Validation Loss: 0.2772 (Best)\n",
      "Epoch   8/20, Loss: 0.2618, Validation Loss: 0.2550 (Best)\n",
      "Epoch   9/20, Loss: 0.2409, Validation Loss: 0.2387 (Best)\n",
      "Epoch  10/20, Loss: 0.2234, Validation Loss: 0.2222 (Best)\n",
      "Epoch  11/20, Loss: 0.2066, Validation Loss: 0.2069 (Best)\n",
      "Epoch  12/20, Loss: 0.1924, Validation Loss: 0.1968 (Best)\n",
      "Epoch  13/20, Loss: 0.1793, Validation Loss: 0.1907 (Best)\n",
      "Epoch  14/20, Loss: 0.1679, Validation Loss: 0.1820 (Best)\n",
      "Epoch  15/20, Loss: 0.1574, Validation Loss: 0.1718 (Best)\n",
      "Epoch  16/20, Loss: 0.1482, Validation Loss: 0.1588 (Best)\n",
      "Epoch  17/20, Loss: 0.1396, Validation Loss: 0.1525 (Best)\n",
      "Epoch  18/20, Loss: 0.1319, Validation Loss: 0.1455 (Best)\n",
      "Epoch  19/20, Loss: 0.1248, Validation Loss: 0.1528 (Patience: 1/5)\n",
      "Epoch  20/20, Loss: 0.1180, Validation Loss: 0.1410 (Best)\n",
      "Restored best model state\n",
      "Evaluating on training set...\n",
      "Loss: 0.1121, Accuracy: 0.9672\n",
      "Evaluating on validation set...\n",
      "Loss: 0.1410, Accuracy: 0.9592\n",
      "Evaluating on test set...\n",
      "Loss: 0.1359, Accuracy: 0.9595\n"
     ]
    }
   ],
   "source": [
    "model1 = SimpleNet().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model1.parameters(), lr=0.001)\n",
    "\n",
    "epochs = 20\n",
    "train_model(model1, train_loader, val_loader, loss_fn, optimizer, epochs, device)\n",
    "\n",
    "print(\"Evaluating on training set...\")\n",
    "evaluate_model(model1, train_loader, loss_fn, device)\n",
    "\n",
    "print(\"Evaluating on validation set...\")\n",
    "evaluate_model(model1, val_loader, loss_fn, device)\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "evaluate_model(model1, test_loader, loss_fn, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c601924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight:\t100352\n",
      "fc1.bias:\t128\n",
      "fc2.weight:\t8192\n",
      "fc2.bias:\t64\n",
      "fc3.weight:\t640\n",
      "fc3.bias:\t10\n",
      "Total parameters: 109386\n"
     ]
    }
   ],
   "source": [
    "# Print layers and the number of parameters in each layer\n",
    "total = 0\n",
    "for name, param in model1.named_parameters():\n",
    "    print(f\"{name}:\\t{param.numel()}\")\n",
    "    total += param.numel()\n",
    "print(f\"Total parameters: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5eca036a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/20, Loss: 0.3686, Validation Loss: 0.1510 (Best)\n",
      "Epoch   2/20, Loss: 0.2035, Validation Loss: 0.1120 (Best)\n",
      "Epoch   3/20, Loss: 0.1695, Validation Loss: 0.1041 (Best)\n",
      "Epoch   4/20, Loss: 0.1643, Validation Loss: 0.0994 (Best)\n",
      "Epoch   5/20, Loss: 0.1541, Validation Loss: 0.0942 (Best)\n",
      "Epoch   6/20, Loss: 0.1499, Validation Loss: 0.0944 (Patience: 1/5)\n",
      "Epoch   7/20, Loss: 0.1436, Validation Loss: 0.1004 (Patience: 2/5)\n",
      "Epoch   8/20, Loss: 0.1437, Validation Loss: 0.0944 (Patience: 3/5)\n",
      "Epoch   9/20, Loss: 0.1453, Validation Loss: 0.0963 (Patience: 4/5)\n",
      "Epoch  10/20, Loss: 0.1431, Validation Loss: 0.0878 (Best)\n",
      "Epoch  11/20, Loss: 0.1427, Validation Loss: 0.0884 (Patience: 1/5)\n",
      "Epoch  12/20, Loss: 0.1405, Validation Loss: 0.0935 (Patience: 2/5)\n",
      "Epoch  13/20, Loss: 0.1391, Validation Loss: 0.0897 (Patience: 3/5)\n",
      "Epoch  14/20, Loss: 0.1408, Validation Loss: 0.0877 (Patience: 4/5)\n",
      "Epoch  15/20, Loss: 0.1385, Validation Loss: 0.0928 (Patience: 5/5)\n",
      "\n",
      "Early stopping triggered at epoch 15\n",
      "Best validation loss: 0.0878\n",
      "Restored best model state\n",
      "Evaluating on training set...\n",
      "Loss: 0.0566, Accuracy: 0.9841\n",
      "Evaluating on validation set...\n",
      "Loss: 0.0928, Accuracy: 0.9722\n",
      "Evaluating on test set...\n",
      "Loss: 0.0840, Accuracy: 0.9751\n"
     ]
    }
   ],
   "source": [
    "# Model with regularization\n",
    "model2 = SimpleNetWithRegularization(dropout_rate=0.2)\n",
    "model2.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model2.parameters(), lr=0.001, weight_decay=0.001)\n",
    "\n",
    "epochs = 20\n",
    "train_model(model2, train_loader, val_loader, loss_fn, optimizer, epochs, device)\n",
    "\n",
    "print(\"Evaluating on training set...\")\n",
    "evaluate_model(model2, train_loader, loss_fn, device)\n",
    "\n",
    "print(\"Evaluating on validation set...\")\n",
    "evaluate_model(model2, val_loader, loss_fn, device)\n",
    "\n",
    "print(\"Evaluating on test set...\")\n",
    "evaluate_model(model2, test_loader, loss_fn, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af1d9176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight:\t100352\n",
      "fc1.bias:\t128\n",
      "bn1.weight:\t128\n",
      "bn1.bias:\t128\n",
      "fc2.weight:\t8192\n",
      "fc2.bias:\t64\n",
      "bn2.weight:\t64\n",
      "bn2.bias:\t64\n",
      "fc3.weight:\t640\n",
      "fc3.bias:\t10\n",
      "Total parameters: 109770\n"
     ]
    }
   ],
   "source": [
    "# Print layers and the number of parameters in each layer\n",
    "total = 0\n",
    "for name, param in model2.named_parameters():\n",
    "    print(f\"{name}:\\t{param.numel()}\")\n",
    "    total += param.numel()\n",
    "print(f\"Total parameters: {total}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
